neuralIsing
===========

The 2-dimensional square Ising model is investigated with a variational autoencoder in the non-vanishing field case for the purpose of extracting the crossover region between the ferromagnetic and paramagnetic phases. The encoded latent variable space is found to provide suitable metrics for tracking the order and disorder in the Ising configurations that extends to the extraction of a crossover region in a way that is consistent with expectations. The extracted results achieve an exceptional prediction for the critical point as well as favorable comparison to the configurational energetics of the model and agreement with previously published results on the configurational magnetizations of the model. The performance of this method provides encouragement for the use of machine learning to extract meaningful structural information from complex physical systems with no known order parameters.

Requirements
------------

- numpy
- numba
- tqdm
- dask (or joblib)
- scikit-learn
- scipy
- keras
- tensorflow (or theano)
- matplotlib

ising.py
--------

This script allows for simulation of the 2-dimensional Ising model across a range of external fields and temperatures. This is done with a standard Monte Carlo (MC) simulation where the MC moves consists of an attempt at flipping a spin. with replica exchange Markov chain Monte Carlo (REMCMC) moves to aid in equilibration. All of the configurations are of the same square shape. The interaction strengths and magnetic moments are tunable. Simulations can be restarted from checkpoints in older simulations.

Example usage:

python ising.py -v -p -c -nw 16 -nt 1 -mt fork -n init -ls 8 -j 1.0 -mm 1.0 -hn 17 -hr -2 2 -tn 17 -tr 1 5 -sc 1024 -sn 1024 -rd 128 -rc 0

This is a run starting from random configurations (no restart) in verbose mode (-v) and parallel mode (-p) with a dask client(-c) using the forking method (-mt) with 16 workers (-nw) that each have 1 thread (-nt). The name of the simulation is 'init' (-n) and is used for the outpur file prefix. The configurations are square lattices of side length N=8 (-ls). The interaction coefficients and magnetic moments are set to unity (-j and -mm). Simulations are generated over a range of 17 external field strengths (-hn) ranging from -2 to 2 (-hr) and 17 temperatures (-tn) ranging from 1 to 5 (-tr). 1024 samples will be generated (-sn) for each external field and temperature combination. A sample is generated after attempting 8*N**2 spin flips. None of the samples will be dumped to output, however, since the sample cutoff is also 1024 (-sc), which determines after which step samples will be recorded. A REMCMC move is performed after each step is completed. No REMCMC moves will be performed after the cutoff (-rc). Snapshots of the samples will be dumped every 128 steps (-rd), from which simulations can be restarted.

python ising.py -v -p -r -nw 1 -nt 16 -rn init -rs 1024 -n run -ls 8 -j 1.0 -mm 1.0 -hn 17 -hr -2 2 -tn 17 -tr 1 5 -sc 0 -sn 1024 -rd 128 -rc 1024

This will then start a simulation named 'run' (-n) restarting (-r) from the dump generated by the prior run 'init' (-rn) at step 1024 (-rs). The run will be parallel, but in the absence of the dask client flag (-c), the joblib library will instead be used in the threading mode, thus there is only 1 worker (-nw) with 16 threads (-nt). All 1024 steps (-sn) will be recorded since the cutoff (-sc) is 0. No REMCMC moves will be performed since the cutoff (-rc) is 1024.

Use the help flag (-h) for more options.

parse.py
--------

This script simply parses the output from 'ising.py' into numpy arrays. There are flags for using verbose mode (-v), setting the name of simulation whose output will be parsed (-n), and the lattice size for the configurations in the system (-ls).

Example usage:

python parse.py -v -n run -ls 8

TanhScaler.py
-------------

This script implements a scikit-learn-style hyperbolic tangent feature scaler. This scaler uses the hyperbolic tangent of the standard scores that would be given by a standard scaler in order to enforce a common feature range across the feature space. This method is far less vulnerable to outliers than the minmax or standard scaling methods.

vae.py
------

This script uses a variational autoencoder (VAE) to produce a latent encoding of the Ising configurations. It is found that these latent encodings contain information about the phase that the configurations belong to. This is consistent with the assumption that a change in physical phase is accompanied by some pattern change in the measured data. Detecting these pattern changes happens to be a particular strong suit of machine learning methods. This approach also has the advantage of requiring very little a priori physical information before analysis aside from the raw configurational data, which may prove to have applications in studying more complicated physical systems.

A variational autoencoder is a type of autoencoder that applies a constraint to the latent variables. As with any autencoder, a VAE involves feeding an input into a neural network that encodes the input into the latent variables, which is then fed into a neural network that decodes the latent variables back to the original input. In this way, a VAE can be seen as a neural network equivalent to unsupervised machine learning methods involved in feature reduction. Ideally, through learning to efficiently encode inputs to latent variables that then can be reliably decoded to the original input, the latent variables will be representations of important structural features of the inputs. In essence, this is a method for feature space reduction. The VAE, however, introduces an additional constraint on the latent variables. Two different arrays will be used to define the latent space, respectively representing the means and standard deviations of a multivariate Gaussian distribution. The input for the decoder is then a random variable sampled from a Gaussian distribution determined by these means and standard deviations. In order to ensure that the latent variables fulfill their prescribed roles, a Kullback-Liebler divergence term is added to the standard reconstruction loss. The Kullback-Liebler divergence is a calculation of how well two different distributions resemble each other, with a value of 0 implying the distributions are identical. By minimizing the Kullback-Liebler divergence, the latent variables can be constrained to approximate Gaussian parameters.

After obtaining the latent variables for the entire set of Ising configurations, principal component analysis (PCA) is applied to the means and standard deviations independently. This is done in order to maximize the statistical variance of the samples across some linear combination of the original variables through an orthogonal linear combination.

Example usage:

python vae.py -v -pt -g -n run -ls 8 -si 2 -sn 256 -sc global -bk tensorflow -pr -ld 2 -opt sgd -lr 1e-3 -lss mse -ep 32 -bs 32 -sd 256

This will calculate the results of the VAE encodings of the Ising configurations in the verbose mode (-v) with plotting the results enabled (-pt). The gpu flag (-g) will run the analysis in gpu mode, but the parallel flag (-p) can be used in absence of a gpu where the number of threads (-nt) can be additionally specified. This analysis will be done for the output from the simulation named 'run' (-n) with a Ising configuration lattice size of 8 (-ls). Every 2 external fields and temperatures will be sampled (-si) with 256 randomly chosen samples for each combination (-sn). The global scaler will be used (-sc), which scales the global minimum and maximum of the Ising configurations to the feature range [0, 1]. Other scalers such as minmax, standard, robust, and tanh are also available, which are performed independently on each lattice site across the configurations. No scaling can also be used by specifying 'none'. The tensorflow backend (-bk) will be used, but the theano backend is also supported. PReLU activations will also be used in favor of ReLU or linear activations present in the autoencoder (-pr). The latent dimension is set to 2 (-ld) and the sgd optimizer is being used (-opt) with a learning rate of 1e-3 (-lr) on the mean squared error loss function (-lss). Other optimizars are available, such as adadelta, adam, and nadam. Additionally, there are also more choices for the reconstruction losses, including binary crossentropy, mean absolute error, and logcosh losses. Training will be done for 32 epochs (-ep) with a batch size of 32 (-bs) and a random seed of 256 (-sd). Repeated computation of different steps in the program is avoided by loading relevant data that was dumped in prior runs using the run parameters to uniquely identify the file names.

If the results are plotted, many plots are generated. The distribution of the errors from the VAE decodings is plotted as well as the distribution of the Kullback-Liebler divergences for the latent variables. Diagrams for the raw latent variables as well as the principal components of said latent variables (done independently for mu and sigma) are plotted as well with respect to the external field strengths and temperatures.